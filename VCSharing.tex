\documentclass[10pt,journal]{IEEEtran}
\usepackage{ctex}
\usepackage{amsmath}
\usepackage{comment}


\begin{document}
\title{端口间缓存和vc共享技术研究}
%\onecolumn
%\author{\IEEEauthorblockN{Baoliang Li}}

\author{Baoliang~Li, %~\IEEEmembership{Student Member,~IEEE,}
        Zeljko Zilic, %~\IEEEmembership{Senior Member,~IEEE,}
        Wenhua~Dou, %~\IEEEmembership{Non-Member,~IEEE}% <-this % stops a space
\thanks{Baoliang~Li and Wenhua Dou are with the College of Computer Science, National University of Defense Technology, Changsha 410073, P.R. China}%
\thanks{Zeljko Zilic are with Department of Electrical \& Computer Engineering, McGill University, Montreal H3A-2A7, Quebec, Canada}%
\thanks{Manuscript received XX XX, 2014; revised XX XX, 2014.}}

\markboth{Journal of XXX,~Vol.~XX, No.~XX, XX~2014}%
{Li \MakeLowercase{\textit{et al.}}: 端口间缓存和vc共享技术研究}

\maketitle

\begin{abstract}
本文提出了种基于端口间缓存共享的方法。
\end{abstract}
\begin{IEEEkeywords}
Networks-on-Chip (NoC), buffer sharing
\end{IEEEkeywords}

\section{Introduction}
引言的写作思路：
（1）通常的路由器结构是每个输入端口有自己的缓存；这些缓存资源被组织成不同的虚通道。
（2）这种路由器的结构存在很大的问题，最关键是缓存的利用率不均衡而且很低。同一个端口内不同VC的利用率不同；不同端口间的缓存资源利用率也不同。
（3）针对前一种现象，ViChaR被提出；之后Lai提出另一种基于链表的等效而且低成本的实现方式；
（4）针对后一种情况，xx提出了partial sharing的方法，该方法假设某些路由端口之前有一部分FIFO是可以被两个端口共享的；但是，在任意时间该共享FIFO里只能存储一个端口的报文切片，当该报文的后续切片被阻塞时，该共享FIFO的利用率非常低；MH提出了一种细粒度的端口间的缓存共享（两篇论文）的方法；
（5）以上的两种方法由于支持的VC数可以很多，导致VC和SA仲裁非常复杂；另外这种方法都具有一个共同的问题：需要大量的控制存储资源来支持动态的VC分配和使用。
（6）另外，以上提出的两种提高缓存利用率的方法都是基于输入缓存的路由器结构；在这种结构中通常采用VC和SA仲裁来消除冲突，由于Allocator的匹配效率很低，导致路由器的吞吐率不高；相反，输出缓存的路由器具有很高的吞吐率，但是需要Crossbar和Memory有一定的加速比，难以在片上实现。在xx中提出一种DBS结构，这种结构采用分布式的存储器来模拟输出排队，在xx情况下可以达到97\%的吞吐率。但是，这种结构是另一种缓存共享的结构，可以极大的改善输入排队路由器的吞吐率。尽管分布的缓存资源可以被所有的端口使用，实现了端口间的缓存共享。但是，这种结构的缓存利用率依然不高，我们举例说明如下。

Proper buffer sizing and organization are essential to achieving optimal network performance

画2张图表示静态缓存分配存在的问题：（1）3x3 mesh的例子说明端口之间的利用率不平衡；(2)同一个端口内VC之间的利用率不平衡。以3x3的mesh网络为例，在热点流量情况下（中间节点为热点）来说，热点的N和S端口接受的流量是W和E的3倍。因而如果能让W和E端口使用更多的buffer，那么那么对于因减少流量控制造成的stall而引起的大的延迟是非常有好处的。最理想的办法是，令每个端口使用$B_p=B_{total}\times\frac{C_p}{\sum_{p=1}^NC_p}$的buffer.然后对于每个端口所能使用的$B_p$个slot进行动态的管理。此外，端口间的buffer共享还有利于改进公平性以及不同数据流的吞吐率。最后指出，只有将端口间和VC 间的缓存共享同时实现才能够提高和改进buffer的利用率，以实现用最少的buffer来实现最高的性能。因为input buffers account for a large fraction of the overall area and power budget of typical Network-on-Chip。

再画一张图表示当前动态VC分配所面临的信约耗尽问题。并指出Lai等人提出的拥塞避免算法可以避免端口间的拥塞，但是端口内同一个VC的拥塞却无法解决。Standford大学的方法可以解决这一问题。

当把这种共享缓存形式的路由器应用于CPU-GPU混合的系统中时，带来的问题会更大，主要原因在于：GPU流量很大会很快将VC耗尽，而CPU流量很小由于信约太少而导致过大的延迟。而CPU程序通常是延迟敏感的。

另外，完全动态的VC分配还会增加VCA和SWA的压力，而这两个流水段通常又处于路由器的关键路径上。

再画1张图表示当前动态方案可能面临的死锁问题以及相应的解决办法。

对相关研究现状的描述：Lai和VichaR的方法只解决了端口内的VC动态分配问题,此外DAMQ的方法访存周期较长（3拍），VCR的方法也只适用于同一个端口内，而且所解决的是在故障情况下的性能平滑下降问题。对于端口间的动态缓存管理，有一类方法方法采用完全共享的centralized buffer，这类方法属于精粒度的共享；还有一类方法采用不完全共享（DAC11），但是该方法不支持VC。而MH的方法只有在某个端口发生故障的情况下其对应的buffer才能够被其它端口使用，而且这种方法是全动态的，因而硬件开销很大。采用链表的方式实现的完全VC共享会消耗大量的存储资源和芯片面积，standford大学的研究结果表明这类方式虽然提高了资源的利用率但是开销是十分具大的，以16个VC为例，大概会引起60\%的开销。另外一方面，完全静态的方式又会严重影响性能。

对我们的方案的简单介绍：因而我们的方法是在二者之间进行折衷，采用部分动态的VC 共享方式，每个端口分配一定数量的容量较小的静态VC（大小为可实现100\%吞吐率的最小值），端口之间共享一定数量的动态VC。每个端口只有当新到达的报文与被阻塞的报文不属于同一条流而且占用动态VC的数量小于其配额时才会启用动态VC。

我们方案对上述几个问题的解决方法：（1）当某个端口的流量较少时，静态分配的VC足够支持切片数据的无阻塞移动。而当某些端口的流量较多时，共享的buffer则可以用来缓存大量的切片。由于静态分配的VC相当于是为每个端口预留的配额，因而可以有效的避免共享缓存被某个端口耗尽时对低速端口的影响。（2）同时，为了减轻VCA段的压力，我们可以还采用VVC映射的方法来动态的调整对动态缓存的使用。因为VCA通常处于关键路径上，这样的做法可以不影响路由器的关键路径。另外，我们所有的新增的硬件都可以并行的独立的操作，而而不对路由器的关键路径产生不利的影响。（3）由于对静态VC的访问消耗的能量要比动态VC要少，因而我们的设计的功耗效率也要比MH的方法要好。（4）对于死锁，我们的方案由于 存在静态配置的VC，因而采用合理的分配方案则可以实现死锁避免，而在MH的方案中，死锁是通过逃逸通道来避免的。

当前几乎所有的路由器都是输入排队的，因为输出排队需要每个端口以P倍的加速比运行才能达到100\%的吞吐率，而输入排队的路由器通常只有不到80\%的吞吐率。另外，输出排队由于需要复杂的仲裁等机制，功耗通常是输入排队系统的1.5倍以上。

利用链表的好处是可以充分利用存储资源，但是缺点是硬件开销太大。因此对于这些共享的存储体我们有以下几种组织方式。对每个memory bank设置一1个存储控制模块这样使用的总的控制寄存器的数量要比为每个端口所有的存储体设置一个共用控制器要节省很多的硬件资源。因此，我们的方法从两方面节省了开销，一方面是几个小的控制器；另一方面是我们提出的混合式的存储管理方法的动态存储容量要比同等大小的完全动态的管理方式要小很多。因，最后可以画一张表并给出一张图列不这些比较的结果。

我们的SWA要进行修改，由于属于同一个输入端口的不同的存储体可以支持并行的读写，因此SA段可以进行修改以支持这个特性。在通常的路由器中，每个输入端口有1个$V_s:1$的仲裁器用于选出该输入端口胜出的VC（在这一阶段解决了FIFO读冲突），每个输出端口有1个$P:1$的仲裁器用于选择最终胜出的端口（在这一阶段解决了FIFO的写冲突）；在MH的方案中，每个输入端口有1个$V_{max}:1$ 的仲裁器，每个输出端口有1个$P:1$ 的仲裁器；而在我们的方案中第一阶段有1个$V_s:1$和N个$V_d:1$的小的仲裁器组成（$V_s+N*V_d=V_{max}$）；而第二阶段每个输出端口有1个$(5+N):1$ 的仲裁器。通常逻辑综合可以发现我们的方案由于每个仲裁器都比较小，因而可支持的最高时钟频率也最大，另外硬件开销也可以进行一些比较。

还有就是硬件开销，由于MH的方案每个端口有一个较大的缓存器和一套控制逻辑，假设每个端口有B的缓存资源；那么free buffer tracker的开销为$B\times log_2 B$，header pointer 的开销是$V_{max}\times log_2 B$，tail pointer的开销是$V_{max}\times log_2 B$，下一跳指针的开销$B\times log_2 B$；5个端口总的控制开销为$10\times(B+V_{max})\times log_2B$；而在我们的方案中，由于总的缓存资源中有一部分会被独立分配给每个端口做为静态VC，而且剩下的动态缓存资源又被分为几个小的memory bank,每个memory bank需要的控制资源比较少，我们假设有一半的缓存分配给静态VC，那么我们的方法控制开销为$5\times(B+2V_{max})\times log_2(B/2)$；

另外，我们采用的VC重命名技术也在一定程度上简化了VCA的实现。关于VCA的实现，传统的方法在输入端采用V:1仲裁器，在输出端采用PV:1仲裁器；MH的方法输入端采用P个$V_{max}:1$ 仲裁器，输出端采用1个P:1仲裁器；而我们的方法，在输入端采用n个$V_i:1$仲裁器，其中（$\sum_{i}^nV_i=V_{max}$）,在输出端采用nP:1 仲裁器。采用几个小的仲裁器并行工作来代替一个较大的仲裁器的好处是降低硬件复杂度并易于提高主频。（考虑到同一个端口之间不需要进行匹配，能否利用n(P-1):1来代替）由于我们的方法在输入端有更多的请求胜出，因而更加有利于提高匹配的成功率。由于我们的方法在VCA以及SWA上都有较高的匹配成功率，可以预见我们的路由器可以提供较高的吞吐率。每个下游节点的memory bank配置发生变化时都会通知VCA，VCA据此来屏蔽不能用的VC段。这样就不会存在冲突了。

由于每个memory bank有独立的操控逻辑，因而我们支持的最大VC数可以比MH的方法要大。

VCA也需要修改。假设每个端口有m个静态VC，那么VCA的实现规则是对于所有请求，优先分配这m个静态vc，对于暂时没有被分配给静态VC 的请求，则为其分配一个各不相同的标识符VCID，通过该VCID在下一级路由器中再进行动态的VC分配。我们称这一个步骤为VC重命名技术，下一级可分配的动态VC数量与其可使用的动态bank 数有关。通过使用VC重命名技术，我们的设计极大的简化了VC分配器的复杂度，这也为提高系统的主频带来很大的好处。因为VCA通常在关键路径上。这样一来，我们的VCA的实现电路得到了极大的简化，画一张图给出对应的电路设计方法。为了支持这一改变，信约控制模块也要进行相应的改变，有m+1个信约计数器。注意：VC的重命名技术可能已经在MH和Lai的论文就已经实现。

我们的VA和SA都是基于优先级的，VA阶段优先分配静态VC，SA阶段优先仲裁静态VC。

有可能在为报文指定Memory bank时采用一定的策略，以提高其吞吐率。

这些共享的存储体每一个都独立的检测不同输入端口的流量状态（这些流量状态由若干个统计计数器来实现），并通过协商来确定每个存储体的分配方式以及分配的实际地址段。再加上我们的SWA段支持同一个端口的多个bank并行输出，因而有希望得到更高的吞吐率。在设计的过程中需要注意free-buffer-tracker的实现方法，这涉及到某个bank能否很快被别的端口使用。但是同时也会对系统的吞吐率有一些影响。

之前MH的方案还有下面的问题：由于每个端口的报文或者都存在一个memory bank里，或者与其它端口的报文存储在一起。这样的方法有以下问题：（1）去往不同端口的报文无法并行传输；我们的方案采用多个并行的存储体以减少这一影响。画图展示这一影响。实际上就是一种特殊的HoL。假设端口W到达的切片目的地为E和L，端口N和S都只产生目的端口为E的报文；在RR调度方式下，平均每3个周期服务1个W的E报文；而实际上，W端口中向L端口的切片在高度向E端口的切片时是无法前进的，由于读冲突的存在。

片上网络中存在的几种形式的阻塞：因VC分配失败造成的阻塞对性能的影响最大，之间提出的动态VC分配实际上是在降低因为这一种形式的阻塞进而提高吞吐率降低延迟的。但是，这类动态VC分配方案的本质是为每个端口分配大量的VC，并实现同一个端口内的不同VC的动态缓存分配（为了防止死锁，每个VC需要至少保留一个缓存空间,剩余的缓存空间则可以在不同的VC之间进行动态的分配。) 实际上，造成VC阻塞的原因是大量不完整的报文（即尾切片还没有到达）占据了所有的VC。存在大量不完成报文的原因主要是因为阻塞造成的同一个报文的不同的切片跨跃存在于几个路由器中。换句话说，这些路由器中都会有一条VC被该报文所占据而无法被其它报文所利用。由于不同端口的流量情况不同。但是，某个端口的不完全报文的最大数量等于于上一级的所有路由器中需要向该路由器注入流量的VC的总数。VC保证在同一个通道中的报文切片顺序不会交错。

我们的方案为每个端口预留一定量的VC，然后剩余的VC被所有的端口共享。这样做的原因是为了为每个端口保证一定的吞吐率和带宽以避免当该端口出现猝发流量时没有可用VC的情况。预留不同数量的VC最终的性能是不同的，对于不同的流量模式可以进行多次实验找出最优的组合。目测发现这种部分虚通道共享方案对于热点流量效果会比较好，而热点流量又恰好对应于CMP系统中的MC或home节点。

之后MH提出的端口间缓存共享的方案实际上实现了端口间的缓存共享,以减少因流量控制而造成的吞吐率下降和延迟增加。但是因为流量控制而造成的阻塞造成的影响相对虚通道不足造成的影响较小。

我们提出的方案同时实现了端口间的缓存共享和虚通道共享，特别是端口间的虚通道共享对改进系统的吞吐率是非常有好处的，因为VC阻塞通常要较多的时钟周期。我们在做实验的时候还可以考虑这样一种情况：对于conventional router、damq路由器和我们提出的路由器结构而言，为了达到同样的性能，我们的方案在需要的缓存资源方面的节省情况。由于实现了端口间的VC共享，我们的方案还可以实现的另一个目标是减少每个VC仲裁器的开销。因为这个仲裁器通常处于路由器流水线中的关键路径上。为了采用较小的分配器来实现大量动态VC的分配，我们的方案要求每个输入端口要有一个VC分发逻辑。该逻辑还要实现VC的重定向以实现流量隔离，避免某个恶意数据流占据所有的缓存空间。

我们的方案具体如下：对于之前的共享VC方案，假设原来的动态VC方案中每个端口有x个VC，那么我们的方案每个端口设置5x/6个VC，另外还有5x/6个VC为各个端口间共享的。与之前的方案，对于同样的VC数量，每个VC仲裁器的复杂度则有所下降，而SW仲裁器的复杂度暂时无法分析得之。采用VC共享方案的本质原因是为了尽可以避免VC阻塞造成的性能下降。对于共享的VC我们称之为影子VC。原因是VC仲裁器没有专门的请求端口，而且与每个端口的对应vc共用一个请求端口，因为被分配的VC在尾切片没有到达之前是不会grant任何请求的，因而该端口可以被影子vc所使用。在我们的方案中，每个端口所能使用的vc数量在5x/6和10x/6之间，远远大于之前的设计方案，但是所用的vc分配器规模则远远小于之前的方案。实现这一目的的根本原因在于我们实现了vc arbiter的复用。为了区分这两类vc，我们需要在原来的channel总线中增加一根线表明当时到达的切片的vc id是对应于影子vc还是本原vc的。另外，我们还需要在channel总线中增加5x/6根线来表明每个影子vc是否可以被该端口用于vc分配。这些线由下流路由器中一个集中的控制逻辑驱动，该控制逻辑会根据每个端口对应的私有vc的占用情况来决定对应的影子vc是否分配给这个端口。5x/6个共享vc也是动态的分配缓存空间，为了避免写冲突，要求这些缓存空间被分成若干个小的存储体，这些存储体可以被并行的读写。每个存储体有自己的sw arbiter，因而我们的方案中sw allocator也需要重新设计。由于crossbar的端口数量也有所增加，因而也会很大程度上提高吞吐率。总起来说，我们的方案与conventional router相比实现的性能提升源于三个方面：（1）实现了动态的vc分配，提高了缓存的利用率，可以利用较少的缓存资源实现同样的性能；（2）实现了vc分配器的共享，进而降低了va的复杂度和实现成本；（3）实现了端口间的vc共享，通过发现不同端口间对vc数量的不同需求和共享，我们的方案实现了端口间vc共享进而降低了因vc不足造成的阻塞；（4）我们的方案通过发现不同端口间对缓存容量的不同需要，动态的分配缓存资源到不同的端口，因而实现了资源的高效利用并以此来提高片上网络的性能。与之前提出的动态vc分配的方案相比，我们的方案由于利用了分配器的复用技术，进而实现了利用较小的allocator来实现对大量vc的分配和使用，在很大程度上降低了allocator的实现成本并降低了关键路径长度，对于提高系统的主频非常有好处。为了支持这种vc借用技术，我们的sw分配也需要进行相应的修改。修改后的sw仲裁器也是分为两人个附件，第一个阶段每个端口有1个5x/6:1的仲裁器用于从该端口中选择一个vc作为下一时钟的备选输出vc；另外，共享的5x/6个vc也有一个仲裁器，因此第一阶段我们有6个同样的仲裁器。第二阶段我们在每个输出端口放置1个6:1的仲裁器，用于从每一组仲裁结果中选择出一个vc进行切片传输可以看出，我们的sw分配器第二阶段采用5个相对较大的6:1仲裁器(原来的方案是5:1)，但是在第一附件每个仲裁器的规模都比原来的方案要小（但是原来的方案需要5个我们的方案需要6个）。在具体的实现过程中，我们还可以为共享vc分配器的决策结果赋预较高的优先级以帮助其尽快排空队列以为其它端口所用。另外，我们的vc分配应该只适用于原子vc分配？因为我们的方案中集中的控制器需要根据每个端口的vc的占用情况来动态的分配vc。在最终的实验比较中，我们可以考虑对于同样的性能，三种路由器的实现方案所需要的缓存资源各是多少。另外，我们还可以与mh的方案进行比较，mh的方案为了实现buffer共享，每个端口的next buffer slot ,header ptr和tail ptr应该都是可以寻址到该路由器中任何一个buffer slot的。因此，这种共享方案的控制存储应该是非常耗资源的。

在我们的方案中，为了简化VC分配逻辑，我们在Vc分配和sw分配时给影子vc赋预较高的优先级，以让其尽快排空队列，这样方便其它端口使用。另外，共享的缓存区被分成5个或是5的整数倍数存储体，我们对vc的分配以存储体为粒度，如果某个存储体被分配给某个端口，那么对应于这个存储体的所有的vc也被赋予这个端口。当属于某个端口的共享存储中所有的vc都空闲时该存储体便可以被重新分配。之所以称之为影子vc是因为我们的方案中影子vc与端口的私有vc共享同一个sw端口和va端口，这样即实现了对大规模vc的支持双降低了设计成本。而实际上这样的设计并没有降低系统的性能。

影子vc重新分配的前提是该共享存储体上的所有vc都空间（由于优先分配私有vc因此这种情况会经常出现），集中的控制器根据每个端口的需要进行动态的分配。每个集中的控制器提前若干周期预测每个端口的vc需求然后将vc回收的结果提前告知占用该vc的端口，在收到vc回收请求以后，该端口停止对相应共享vc的再分配，等该vc被排空后该端口向集中控制器告知该共享vc可以进行再分配。集中的控制器是一个有限状态机，该状态机负责对几个共享存储体的分配和调度，每个共享存储体有一个自己的状态机用于协调与各个端口的分配/回收请求。

利用booksim做实验验证每个端口的vc需求量与时间的关系，关画图。测试的方法是为每个端口设置一个极大的vc数量然后在仿真进行过程中记录每个端口实际使用的vc数量。注意，对于不同的vc预设数量所得的结论可能不同。而且极有可能是线性增长的，这个时间我们可以假设每个端口的vc数量为与我们的实验参数相同的静态vc数量，然后看看在不同的流量情况下每个端口实际需要的vc数量。由此引入本论文的主要目的，即通过动态共享vc来降低每个端口报文的vc分配失败造成的阻塞和停顿进而降低平均端到端延迟提升吞吐率。

我们需要强调vc数量与吞吐率间的关系：vc数量越大吞吐率越高（这一点可以利用booksim的仿真结果进行佐证）。因此，我们参数动态共享不同端口的空闲vc资源可以动态的提高系统的吞吐率。我们的方法对吞吐率的改进还体现在我们的方法中共享的缓存有各自的crossbar端口，因而对提高crossbar 的通过率有好处。当然，在最初实现的时候为了修改代码方便可以考虑采用5x5的crossbar然后在进行sw分配前对私有vc和共享vc进行复选。

端口间的缓存共享可以提高buffer的利用率，使得采用较少的buffer实现较高的性能成为可能。但是，完全的端口间缓存共享（例如mh和mit女人）需要非常复杂的逻辑支持以及大量的硬件支持，而这些复杂的硬件又会进一步增加系统的功耗。mit的研究表明尽管共享提高了吞吐率但是功耗却大大增加这样也就在很大程度上降低了这类方案的可用性。我们的方案实际上是在不共享和完全共享之间的一个折衷，通过仅共享一部分缓存空间，我们使用对共享缓存的控制存储开销大大降低，列一个表给出我们的方案与mh方案需要的指针寄存器的数量。最终还可以列一个表给出我们的功耗与mit的方案比较的结果。而且之前的方案都是只共享了缓存而没有共享vc，当某个端口的vc全部被占用时我们的方案还可以借助其它端口的空闲vc。

实现vc共享的目的在于，受限于vc和sw分配器的实现复杂度，每个端口可用的vc数量是非常有限的。另外，受限于系统的设计成本，每个端口的缓存容量也是有限的。而系统所能达到的吞吐率与vc的数量以及缓存容量是正相关的，因此为了提高吞吐率需要较多的vc和缓存空间。但是不同端口的流量特点是不同的，在这种情况下，我们需要一种有效的机制来动态的共享vc和缓存并对其进行高效的分配和使用。之前的方案仅实现了对缓存的动态分配和共享而没有实现vc的共享。然而我们发现因vc不足造成的阻塞要远远比缓存不足造成的后果要严重。而且这种影响随着报文长度的增加而越发明显。vc是维持报文内部切片以正确的顺序传输的有效手段。

在全文的主题上，我们的目标是提高吞吐率降低平均端到端延迟，而之前的很多方案都是提如何提高缓存利用率并降低延迟。

考虑某种情况，当某条流在某个路由器中发生VC阻塞时，该条流会在路由器中积压大量的切片，而切片在缓存区中的积压会将该数据流转变成为恶意数据流。而造成这一现象的原因则是因为虚通道分配失败。因而我们的方案通过避免这种虚通道分配失败来提高吞吐率将对减轻恶意流的影响也会有好处。

A High-Throughput Distributed Shared-Buffer NoC Router应该值得参考。其提高吞吐率的方法与我们的方法很像。类似的文章还有Achieving High-Performance On-Chip Networks with Shared Buffer Routers。但是，这两篇论文中给出的方法都不支持VC，是典型的虫孔路由器。

在后面的研究中可以考虑在动态vc分配的基础上实现流量的隔离和服务质量保证。

有关队列空满的信号由free buffer tracker模块产生。

\subsection{流量控制模块}
\subsubsection{最初的方法}
我们的流量控制模块有两根线，每根线负责其中模块中一个buffer段的信约，信约控制器自动进行信约维护。

我们的方法最重的一个特点是不增加VA和SA段的复杂性，因为这两段都处在路由器的关键路径上。

TAMS的硕士论文中指出，ViChaR的方式提高了一个端口内部的buffer利用率，但是也提高了设计的复杂性和功耗。

DAMQ的思路与ViChaR的区别在于DAMQ采用固定数量的VC，因而会出现HoL。

我们提出的是一种运行的VC自适应技术，之前基于RTC的研究属于面向应用的buffer sizing方法，属于设计时的方法。

ViChaR和RAVC的方法都需要对整个路由器进行重新设计，因而成本高。而我们的新方法则主要是在原来的基础上增加了些硬件来提高buffer的利用率。在做实验的时候我们可以统计每个buffer的利用率。

最终我们要与conventional router进行比较。而且重新设计和实现sink端以统计各种信息。

inter-port buffer sharing是我们论文的主题。

在做实验时可以考虑不同的报文长度以模拟cache一致性协议的需求，例如将报文长度取双峰分布，一个长度为1(对应读请求或者写响应消息)一个长度为16（对应cache行）。

MH提出的RAVC方法虽然支持动态VC数量以避免HoL，但是控制罗男过于复杂，硬件成本过高，需要大量的额外的存储器和寄存器文件来存储控制信息。而我们的方法增加的硬件几乎都是简单的组合逻辑电路，因则硬件成本更低。由于我们采用可扩展的VC结构，因则降低了拥塞端口发生拥塞的可能，在一定程度上对避免HoL也有帮助。

buffer借用在热点流量情况下可能很有用，效果会很明显。而热点流量恰是Cache一致性能及MC访问的重要应用场景。

为了简化硬件设计，我们只允许每个VC被偷一次。

当需要进行VC间缓存共享时，通常的情况是这个端口已经很堵，因则借助端口内的缓存共享来改进性能的性能提升已经非常有限，而端口间缓存共享则没有这个问题。因为应用通常会呈现出流量在同一个路由器的不同端口间的不均衡分布情况，因而采用端口间共享对改进吞吐率有好处。当然，通道间共享和MH的工作都可以有效的防止HoL。因而二者熟优熟劣需要再进行考虑。当然，对于HoL问题，我们的方案可以在选择待偷端口时适当的考虑可能发生的阻塞。对于非均匀流量，特别是热点流量情况下，端口间的流量分布是严格不均匀的。

在进行仲裁时，应当给借用的buffer空间切片以较高的优先级，以方便其清空队列。借用冲突可以通过与victim端口的协商来实现，theif端口和victim端口之间可以通过req/ack来交互。

buffer的读写冲突可以通过独立的两对读写指针来实现，高位读写指针地址以1开头，低位读写指针地址以0开头。

本文的一个重要的目标是改进现在的NoC的buffer利用率，增强性能的同时避免不可接受的硬件开销以及影响关键路径。

我们的设计中，每个VC由两个FIFO组成，prime\_readd\_ptr和prime\_write\_ptr在非共享状态下用最高位做片选，在共享状态下由share信号选择高位地址，其自己的最高位置零。

我们的设计的目的在于同时不引入较大的硬件开销

提出一种新的流量控制方法，两根线，每根线负责维护信约模块中一个FIFO段的可用buffer空间。

之前的buffer共享策略没有考虑到恶意流占用很多buffer的情况，因而会对系统的加速比产生很大的影响。在具体做实验的时候我们可以通过综合流量发送1000个报文所用的时间来进行分析和说明。而我们的方法由于限制每个VC所用的buffer空间最大是3/2，因而会在一定程度上减轻这一影响。

模块划分：

1.流量控制模块：信约生成、信约跟踪。

2.借用buffer的管理模块：处理借用和非借用状态下的地址译码以及读写，仲裁，队列空满检测模块。

3.借且buffer读写控制模块。

4.借用buffer跟踪模块。

5.buffer借用状态机：buffer选择，协商，确认和释放。

我们的共享虚通道技术为每个端口预留少量的VC以减少VC仲裁和分配的压力，为了保证最差情况下的延迟性能，我们需要一种技术来确定每个虚通道所需要的最少的buffer slot数量。这就是我们之前基于RTC的论文所要解决的问题，由于在动态VC共享方案中每个VC只需要保留1个buffer slot便可以保证无死锁。我们的方案采用RTC的方法来确定每个VC所需要分配的最少缓存区。对于热点流量来说，特别是热点处于mesh网络的四个角上或者四条边上的时候，热点路由器和一些其它的路由器总有某些端口的缓存是没有被利用的。不光这些缓存区是没有被利用的，就连这些端口所对应的VC也是无法被有效使用的。因此我们可以考虑共享一部分缓存区和VC，之前MH的方案中VC是每个端口所独有的，没有被共享；为了保证性能，需要为每个端口预留大量的VC才能保证性能。但是这无疑增加了硬件成本降低了资源利用率。共享缓存技术除了增加资源利用率以外，还增加了实现了动态VC分配中间出现的流量隔离技术。

\subsubsection{新想法}
共享的buffer分成几个bank，用以支持动态的VC分配和端口间的缓存调整。用一个状态机或者什么东西来定期的将空闲的bank分配给指定的比较忙碌的端口使用。

如果到达的报文不属于静态VC队头报文的数据流，那么就为他分配一个新的VC，并将该切片写入新的VC，多静态VC处移到动态VC后应当立即向源端返回一个信约，除非动态VC的缓存空间已经已经用完。之后每到达一个报文先看其是否属于某个数据流，如果都不属于，那么就为其分配新的VC，否则就写入相应的VC中去。这样做的目的是防止报文序。

\section{实验与评估}
缓存资源的使用比较：（1）列表比较资源；（2）实验比较性能，包括吞吐率和延迟；（3）综合结果比较面积和功耗。

\section{代码阅读重要提醒}
router\_wrap是路由器的顶层模块，根据不同的配置参数，该模块可以实例化不同的路由器实现类型，例如虫孔路由器和虚通道路由器以及混合路由器。由于我们的研究是基于虚通道路由器的，因此router\_wrap会被实例化为vcr\_top模块。在vcr\_top模块中，为每一个输入端口实例化化一个vcr\_ip\_ctrl\_mac子模块，并为每一个输出端口实例化一个vcr\_op\_ctrl\_mac子模块，整个跌幅器还会实例化一个vcr\_alloc\_mac模块用于进行vc和sw的分配。另外，整个路由器还会实例化一个crossbar模块。

在vcr\_ip\_ctrl\_mac模块中包括的子模块有rtr\_channel\_input和一个rtr\_flit\_buffer模块以及一个rtr\_flow\_ctrl\_output模块，并为每个vc实例化一个vcr\_ivc\_ctrl子模块。

vcr\_channel\_input模块的作用是：在路由器之间进行连接的时候将所有的控制信号和数据信号都汇总在一起用channel总线来表示这样可以简化路由器连接的实现，而这个子模块的作用则是在路由器内部将channel总线中的信号剥离出来以便进行相关的操作。这个子模块的输入信号只有channel总线而输出信号包括：flit\_valid\_out, flit\_head\_out, flit\_head\_out\_ivc, flit\_data\_out和flit\_sel\_out\_ivc。flit\_data\_out和flit\_valid\_out都是经过锁存的输出信号，采用锁存器的目的是实现切片的流水传输。另外，关于切片的vc id需要先对channel的相关位进行译码才能得到选择器能使用的位向量信号。

rtr\_flit\_buffer会根据存储的分配模式确定是采用damq或者FIFO队列的形式来组织缓存。

rtr\_flow\_ctrl\_output模块中，flow\_ctrl\_width等于vc寻址宽度+1（对应于credit\_valid信号）.这个模块输入信号有流量控制有效信号flow\_ctrl\_valid和该事件对应的vc，这两人个信号一个对应于sw分配的授权信号和vc选择信号。输出信号flow\_ctrl\_out连接到上级路由器的rtr\_flow\_ctrl\_input模块中。flow\_ctrl\_out的第0位即credit\_valid由锁存器驱动。信约所对应的vc由select信号译码后得到，该信号也是由锁存器驱动。

vcr\_ivc\_ctrl模块中实例化的子模块包括rtr\_route\_filter,rtr\_routing\_logic和rtr\_next\_hop\_addr.这个模块的实现非常复杂，它的会根据当前的路由器的地址来计算输出端口。这个模拟接受来自rtr\_channel\_input模块译码产生的flit\_valid\_in等信号，另外，这些经过译码的信号还会连接到rtr\_flit\_buffer模块.这个模块用于计算前瞻路由信息并更新头切片的lar域，经过这个模块以后的切片会直接穿过switch并进入rtr\_channel\_output模块。在综合的过程中，这个模块中rtr\_flit\_type\_check模块需要注释掉。这个模块的主要功能包括：（1）跟踪每个ivc的ovc分配情况;(2)为该ivc生成header, tail 的indicator信号.（3）更新该vc的头信息寄存器；（4）解码头切片中的路由信息；（5）计算下一跳的路由信息；（6）跟踪每个信约的使用情况. 对相关信号的说明：flit\_sel\_in信号由rtr\_channel\_input模块译码得到，该信号结合flit\_valid\_in信号可以说明当前到达的切片可以属于该ivc。该模块同时还输出flit\_valid信号，当对应的vc不为空或者该vc有新到达的切片时这个信号有效。表示对应ivc己经被分配的allocated信号在这个模块中被锁存。flit\_tail和flit\_head信号的产生方法是：如果当前队列为空那么就看新到达该vc的切片是否是尾／头切片，否则就根据flit\_buffer的flit\_tail和flit\_head信号。在这个模块中flit\_sent信号非常重要，因为他是决定各个信号取值的关键。这个信号产生的方法是根据vc和sw分配的结果来进行的。当新到达的切片是头切片时，header\_info\_in信号有效，其它包括了该切片的路由等信息，该信号在rtr\_ip\_ctrl\_mac模块中从头切片的前面若干字段中取出得到。该模块中的信约跟踪实现的是对ivc空满状态的跟踪，

rtr\_next\_hop\_addr模块接受的输入包括当前的路由器地址，lar信息和目的地址信息。输出的是下一跳的路由器地址。这个下一跳地址会作为rtr\_routing\_logic模块的输入用于计算当前切片在下一跳路由器处的输出端口。该输出端口信息在经过译码之后写入切片的lar\_header中。

代码中所有以*\_active结尾的信号实际上都相当于是模块使能信号。

vcr\_op\_ctrl\_mac模块中实例化的子模块包括rtr\_flow\_ctrl\_input，rtr\_fc\_state, rtr\_channel\_output, 并为每一个输出vc实例化一个rtr\_ovc\_ctrl。rtr\_flow\_ctrl\_input接受下游路由器的信约反馈信息并对其进行译码和缓存，该模块的两人个输出信号是valid和sel。这两个信号会被输入到rtr\_fc\_state模块，而rtr\_fc\_state模块的作用是跟踪每个ovc的信约使用情况。

rtr\_ovc\_ctrl模块用于跟踪下流路由器的vc使用情况。这个模块实际上只有两个输出信号：flit\_sel和elig。flit\_sel信号最终会连接到rtr\_fc\_state模块上用于跟踪对应vc的信约使用情况。到达该ovc的切片来自正确的端口和ivc时这个信号assert而且被锁存。这个模块根据输入的切片类型以及下流路由器的信约情况来确定对应的ovc是否可用以及对应的空满状态(elig)。这个模块还接受vc和sw分配的结果并依据这些信息进行决策。

rtr\_channel\_output的作用是将各种信号汇总成为channel总线，并实现锁存功能。其作用与rtr\_channel\_input相反。

vcr\_alloc\_mac模块的实现比较简单，其主要功能就是根据配置参数来生成合适的vc和sw分配器。具体的vc和sw分配器实现我们可以不去考虑而直接采用clib库中已经实现的。

Router\_Checker在做综合的过程中要去掉。

VCR是带虚通道的路由器，RTR是SA与VA结合的实现方法，WHR是没有虚通道的实现方式。

代码中的buffer\_size是一个端口的buffer总量，每个VC的buffer数量还需要除以VC数量。

rtr\_flit\_buffer.v输入端口的管理模块

原子分配的时候这种优化策略可能没有效果。一般的VC路由器吞吐率比较高，原子VC分配只有当切片的信约返回时该VC才可用，而一般的VC则在尾切片离开VC时即可以再次进行分配。原子VC分配当VC很多时可以达到很高的性能。

在进行路由器综合时，mesh型拓扑结构中某些边缘路由器的某些端口可以不生成。

代码中的message\_class和resource\_class都是指什么意思？

拓扑结构的连接性包括线、环和全互连。

路由器的端口数量等于每个维度的邻居数乘以网络维度再加上每个路由器的节点数。

代码中的flow\_control\_bypass是什么意思？

link\_ctrl是指功耗控制相关的。flit\_ctrl是指跟flit\_data相关的控制信号，如flit\_head,flit\_tail等信号。

flit\_buffer的寄存器文件的实现方案与mh和lai的方法相同，都是damq的实现策略。这种策略在存储空间较大时需要3个时钟周期才能完成操作，之所以在二者的方案中得到使用是因为他们采用复选器，但是这种方式只适用于规模较小的情况。

随机拓扑的生成可以采用TGFF工具。

注意在VC和SW仲裁时VC非空和满时的选项

VC分配是否偏向空队列的选项

almost full的含义是只剩下1个buffer空间

代码中资源和消息类的含义：为了防止协议死锁，通常需要将网络中的报文分为若干个消息类，并将网络中的vc分几一些子集，然后将不同消息类的报文分配不同的vc子集中。另外，在一个消息类的报文中，为了防止因为资源循环依赖造成的死锁。还可以将每个消息类中的报文分为几个资源类别，不同资源类别的报文在竞争共享资源时形成一个偏序，因而可以避免这种死锁。注意到不同的消息类之间不存储资源死锁，因为他们是被映射到不同的硬件资源上的。在代码中假设每个类别使用的vc数量相同，这样整个路由器中每个端口需要的vc数量便等于资源类型的数量乘以消息类型的数量再乘以每个类型所能使用的vc数量。在具体的实现过程中，我们还要以考虑简化情况即消息类型和资源类型均为1，在这种情况下每个类型的vc数量便等于端口所能使用的最大的vc数量。

\section{代码修改过程}
（1）源端与路由器的接口处：在testbench.v模块中实例化了router\_wrap,flit\_sink.v和packet\_source.v三个模块。为此，我们需要在packet\_source和router\_wrap之间的接口处增加一些信号线以表示这些动态的vc变化信息。具体的方法是：（1）增加M根线从router\_wrap到packet\_source（memory\_bank\_allocated）来表示每个memory bank的可用性；只有当相应的memory bank可用时packet\_source模块可以利用相应的vc。router\_wrap模块按照一定的策略选择该memory bank的下一个使用者并修改memory\_bank\_grant信号，packet\_source发现该信号无效以后便不再分配该vc。当这个memory bank上的所有vc被释放时router\_wrap回收该memory bank并将其分配给其它端口使用。（2）增加一根从router\_wrap到packet\_source的信约反馈信号线credit\_for\_shared用于表示当前反馈的信约是共享vc的还是私有vc的。(3)在packet\_source中增加5个流量控制模块用于跟踪5个共享的存储器的使用情况，这5个共享的存储体上的vc指针只能够寻址1个memory bank。（4）增加一根从packet\_source到router\_wrap的信号线用于表明当前的vc id是针对私有vc的还是共享vc的。由于在源端对输出vc的选择是随机进行的，没有相应的分配模块，因此源端的修改还是非常简单的。另外，由于每个源端只有一条数据流，而且这种数据流是按顺序注入网络的，因此，我们的方案假设在源端不会占用共享的vc。对共享vc的占用仅在路由器内部。由于不会占用共享的vc，因此我们的方案在源端也不需要使用5个额外的流量控制模块。

channel\_width＝链路控制信号的宽度＋切片数据信号的宽度＋切片控制信号的宽度。为了实现我们的设计方案，我们需要增加两人个额外的控制信号memory\_bank\_grant和credit\_for\_shared。

（2）路由器与目的端之间：由于目的端有足够的消耗能力，因此在目的端不存在vc不足的情况。因此，我们的方案在目的端也不会占用共享的vc。对于共享vc的占用仅发生在路由器内部。因此，在目的端也不需要使用5个额外的流量控制模块。

（3）路由器与路由器之间：先要在testbench中增加相应的连接线，然后在router\_wrap内部将这些信号连接到vcr\_top中，然后在vcr\_top中实现这些功能。

通过这种方式，我们的方案所使用的控制存储资源要比mh小，同时我们的方案所用的控制存储资源实际上也比lai的方案要少。在做实验进行比较的过程中我们可以考虑两种情况：（1）同样的flit buffer大小的情况下比较性能； （2）同样的存储控制开销的情况下比较性能。（3）同样的性能情况下两人种方案需要的存储器和控制存储资源的多少。

\section*{Acknowledgement}
The authors thank the reviewers for their suggestions and comments, and all the experiments are carried out at the Integrated Microsystem Lab (IML) of McGill University. This research is supported by High Technology Research and Development Program of China (Grant No. 2012AA012201, 2012AA011902).

\bibliographystyle{unsrt}
\bibliography{Docear}

\end{document}
